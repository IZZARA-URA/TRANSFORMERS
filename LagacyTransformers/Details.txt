@PositionEncoder():
max_seq_len = 80
d_model = 512
pe = torch.zeros(max_seq_len, d_model)
for pos in range(max_seq_len):
    for i in range(0, d_model, 2):
        pe[pos, i] = math.sin(pos / (1000 ** ((2 * i)/d_model)))
        pe[pos, i + 1] = math.cos(pos / (1000 ** ((2 * i)/d_model)))
print(pe)
print(pe.shape)


d_model = 512 
max_seq_len = 80
pe shape = [80, 512a]

register_buffer('pe', pe) : buffer this paremeter but not trainning in optimizing
autograd : keeping record of data(tensors) and all executed opretion